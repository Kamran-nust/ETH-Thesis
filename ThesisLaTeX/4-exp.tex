\chapter{Experiments and Results}

In this chapter, the entire experimental process is presented, from the acquisition of the ground truth dataset (see section \ref{gt}), to the implementation, training and prediction phases of the \modelnameshort\ (see section \ref{impdet}), and finally to the results evaluation and analysis (see section \ref{expres}).

\section{Ground Truth}\label{gt}

Our ground truth dataset consists of satellite images and buildings' coordinates, which are used as inputs and labels respectively when training. In this project, all of the satellite images are collected from Google Static Maps API and all of the latitude and longitude coordinates of the polygon vertices of buildings are collected from OpenStreetMap. For details of the two APIs mentioned above, please refer to subsections \ref{apimap} and \ref{apiosm}.

As mentioned in section \ref{modmer}, the training phase of our model \modelnameshort\ requires two different kinds of ground truth dataset, areas with multiple bounding boxes for FPN and buildings with geometrical shapes for PolygonRNN, all of which are illustrated in subsection \ref{arebui}.

Since the whole dataset is collected from two different sources, the problem of inconsistency may exist. Subsection \ref{proble} describes details of the problems in the ground truth dataset, and subsection \ref{adjust} proposes a solution to adjust the shift between buildings' images and polygons.

\subsection{Google Static Maps API}\label{apimap}

Google Static Maps API\footnote{\lstinline{https://developers.google.com/maps/documentation/static-maps/}} provides an interface that implements maps as high-resolution images. Users can download customized map based on URL with different parameters, which is sent through a standard HTTPS request.

The parameters in URL includes the map type (satellite, roadmap, etc.), latitude and longitude coordinates of the image center, the resolution of the image, the zoom level, and the scale. For the usage of the Google Static Maps API, please refer to section \ref{app:apimap} in appendices. Figure \ref{fig:egsatellite} and \ref{fig:egroadmap} shows two types of images can be obtained by Google Static Maps API.

\input{fig/4-00}

We would have liked to obtain buildings' coordinates from Google Static Maps API as well, but we find that the API does not provide such information directly. Instead, it gives the styled roadmap images like figure \ref{fig:egroadmap}, where querying coordinates requires further corner detection but the boundaries may be still inaccurate (see figure \ref{fig:eghybrid} for example). Thus, this thesis project, only satellite images from Google Static Maps API are used.

\subsection{OpenStreetMap}\label{apiosm}

OpenStreetMap\footnote{\lstinline{https://www.openstreetmap.org/}} provides an interface that implements a map as a XML format \lstinline{.osm} file. The file contains all the information which can be used to recover the map. When the map's bounding box is given, users can retrieve the latitude and longitude coordinates of the buildings' vertices and roads' central lines within the map. For the usage of OpenStreetMap and the format of the \lstinline{.osm} file, please refer to section \ref{app:apiosm} in appendices.

The coordinates obtained by the API is the original latitude and longitude coordinates of the buildings' vertices, which cannot be directly used for training. We need to convert them to relative pixel coordinates with regards to an given satellite image.

As a matter of fact, every fixed point on the earth can correspond to a unique pixel on the map at a specific zoom level. This projection process can be regarded as a function. Since we have already known the relative pixel coordinates of the image center (half width and half height), the projection for an arbitrary point with given latitude and longitude coordinates can be therefore computed. This process can be described as following equations.
\begin{equation}
	(c_x, c_y) = (\frac{w}{2}, \frac{h}{2}) = f(c_{lon}, c_{lat}, z) + (\Delta_x, \Delta_y),
\end{equation}
\begin{equation}
	(p_x, p_y) = f(p_{lon}, p_{lat}, z) + (\Delta_x, \Delta_y),
\end{equation}
where $f$ is the function that projects latitude and longitude coordinates to global pixel coordinates (see \ref{app:projec} in appendices for more details), $c$ is the image center, $p$ is an arbitrary point, the subscripts $x$, $y$, $lon$, $lat$ mean the relative pixel and longitude and latitude coordinates respectively, $z$ is the zoom level, $\Delta$ is the translation constant from the global to the relative pixel coordinate system. Thus, we have
\begin{equation}
	(p_x, p_y) = f(p_{lon}, p_{lat}, z) - f(c_{lon}, c_{lat}, z) + (\frac{w}{2}, \frac{h}{2}),
\end{equation}
for any arbitrary point $p$.

\subsection{Areas and Buildings}\label{arebui}

As mentioned in section \ref{modmer}, \modelnameshort\ is formed by FPN and PolygonRNN. However, the training phase of these two models requires two different kinds of ground truth dataset.

\paragraph{Areas for FPN}
In subsection \ref{modfpn} we have shown that FPN aims at finding rectangular regions of interests. Thus, training FPN generally requires a relatively large image with several objects in it. When considering our problem, an example of the ground truth can be an area of a city with several bounding boxes. Each box contains a single building, regardless of its tight polygon outline. Figure \ref{fig:egareafpn} gives an example area for training FPN and its visualized ground truth label.

\input{fig/4-01}

\paragraph{Buildings for PolygonRNN}
In section \ref{modpoly} we have mentioned that PolygonRNN can only deal with images with single object, meaning that the bounding box of the object should be first given. Thus, training PolygonRNN generally requires a relatively small image with single object, as well as the information of its polygon outline. When it comes to our problem, an example of the ground truth can be a building with some padding and the pixel coordinates of building's vertices. Figure \ref{fig:egbui} gives example buildings for training PolygonRNN and its visualized ground truth label.

\input{fig/4-02}

\subsection{Problems}\label{proble}

Recall that the satellite images and polygons' coordinates are collected from two different sources, the problem of inconsistency may exist. That is to say, the polygon and the actual building we see in the image may not match well in some cases. This inconsistency is mainly reflected in the following aspects.

\input{fig/4-03}

\paragraph{General Shift}
Figure \ref{fig:egshi0} gives two typical shift examples in the dataset. This shift is generally within the range of 30 pixels, either up and down, or left and right.

\paragraph{Polygon Size and Shape}
Sometimes the polygon size and shape is different from the actual building size as well. This is typically because OpenStreetMap measures the bottom of the building and the aerial image is taken from the top. The roof of the house we see from the top can be different from the floor area. Figure \ref{fig:egshi1} shows a building with a smaller polygon and a building with a too rough outline.

\paragraph{Angle Issue}
Some tall buildings may suffer from the angle issue. This is because the satellite cannot always be vertically right above the building, so the side of these very high buildings will be photographed. Figure \ref{fig:egshi2} gives two examples.

\paragraph{Adjacent Buildings}
Some adjacent buildings will be very close together, and some even share a common roof. Thus, from the satellite image, it is generally difficult to distinguish the boundaries between them. However, these buildings are separated in the dataset of OpenStreetMap. It means that where there are originally edges between buildings, it looks like there is no edge in the image. This kind of inaccuracy would have  negative effect on the training phase. Figure \ref{fig:egshi3} and gives two examples.

\subsection{Adjustments}\label{adjust}
Problems such as too rough polygon shape, angle issue and adjacent buildings are unsolvable unless the data sources can correct themselves. What we can improve is to tackle the general shift and the polygon size problem and make the image-polygon pairs more matching. By observation, we can get to some conclusions for a matching image-polygon pair.

\input{fig/4-04}

\paragraph{Color Variance}
The color variance of image pixels covered by polygon generally reaches the local minimum, because the roof of a building usually has only one color or several similar colors.

\paragraph{Edges}
The polygon edges generally lie within the output of the edge detection (e.g. Sobel, Canny) performed on the image.

\paragraph{Corners}
The polygon vertices can generally match the output of the corner detection (e.g. Harris, SIFT) performed on the image.

Based on these conclusions, we propose a brute force shift adjustment method. Briefly, we resize and translate the initial polygon within a certain range, and traverse all the cases to see where the minimum color variance, maximum edge and corner coverage are reached. Figure \ref{fig:egadj} illustrates some examples which compare the polygons before and after the adjustment. Results show that this algorithm can well address the shift problem to some extent.

\section{Implementation Details}\label{impdet}

In this section, several implementation details are given in order to the replicate the model in the future if needed. Subsection \label{datainfo} gives information and some notes of the dataset. Subsection \ref{config} gives the basic configuration of the model. Subsection \ref{trnphs} and \ref{prdphs} focus on the training and prediction phase respectively.

\subsection{Dataset Information}\label{datainfo}

The dataset consists of two cities, Zurich and Chicago, including all the buildings in the range shown in table \ref{tab:bldrange}.

\begin{table}[!h]
	\centering
	\caption[Sampling range of buildings in Zurich and Chicago]{Sampling range of buildings in Zurich and Chicago.}
	\label{tab:bldrange}
	\begin{tabular}{l|l|l}
	\hline
	City & Zurich & Chicago \\ \hline
	East Boundary & E $8.4092916^\circ$ & W $87.8039744^\circ$ \\
	South Boundary & N $47.2879518^\circ$ & N $41.8799477^\circ$ \\
	West Boundary & E $8.6729490^\circ$ & W $87.6721554^\circ$ \\
	North Boundary & N $47.4664863^\circ$ & N $41.97799394^\circ$ \\
	\hline
	Number of Buildings & 96,573 & 228,074 \\
	\hline
	\end{tabular}
\end{table}

Note that buildings with more than 20 or less than 4 vertices are excluded. The building images are all squares but in different sizes, and include paddings. All polygons are set to be clockwise, and in each polygon, three consecutive vertices cannot be collinear. If so, the second vertex is removed.

Aerial images are taken based on the information shown in table \ref{tab:arerange}.

\begin{table}[!h]
	\centering
	\caption[Sampling range of areas in Zurich and Chicago]{Sampling range of areas in Zurich and Chicago.}
	\label{tab:arerange}
	\begin{tabular}{l|l|l}
	\hline
	City & Zurich & Chicago \\ \hline
	Center Longitude & E $8.5468221^\circ$ & W $87.7380649^\circ$ \\
	Center Latitude & N $47.3768587^\circ$ & N $41.9289708^\circ$ \\
	Step Longitude & $4.023094^\circ\times10^{-4}$ & $4.469772^\circ\times10^{-4}$ \\
	Step Latitude & $2.724221^\circ\times10^{-4}$ & $3.324594^\circ\times10^{-4}$ \\
	Horizontal Step Range & (-144, 144) & (-144, 144) \\
	Vertical Step Range & (-144, 144) & (-144, 144) \\
	Zoom Level & 19 & 20 \\
	Scale & 1 & 1 \\
	Image Size & (600, 600) & (600, 600) \\
	\hline
	Number of Areas & 57,741 & 77,716 \\
	\hline
	\end{tabular}
\end{table}

Note that areas without any buildings are excluded. Besides, there is an overlap between neighboring images because each building is required to appear completely at least once within all area images. For example, in figure \ref{fig:egarea}, for the incomplete buildings locate near the image top edge, we can find their complete shapes in its northern neighboring area image.

\subsection{Model Configuration}\label{config}

Table \ref{tab:configpara} shows the parameters used in our model.

\begin{table}[!h]
	\centering
	\caption[Configuration parameters.]{Configuration parameters}
	\label{tab:configpara}
	\begin{tabular}{l|l}
	\hline
	Item & Value \\
	\hline
	Area Image Resize & (256, 256) \\
	Building Image Resize & (224, 224) \\
	Resolution of Output & (28, 28) \\
	\hline
	RNN Max Sequence Length & 20 \\
	RNN Number of Layers & 3 \\
	LSTM Number of Output Channels & (32, 16, 8) \\
	\hline
	Number of Layers of Feature Pyramid & 4 \\
	Anchor Scale & (16, 32, 64, 128) \\
	Anchor Shapes & (1:4, 1:2, 1:1, 2:1, 4:1) \\
	Feature Shapes & ($64\times64$, $32\times32$, $16\times16$, $8\times8$) \\
	Feature Stride & (4, 8, 16, 32) \\
	\hline
	\end{tabular}
\end{table}

\subsection{Training Phase}\label{trnphs}

Table \ref{tab:trnphs} shows the configuration during the training phase of the model.

\begin{table}[!h]
	\centering
	\caption[Configuration during training phase.]{Configuration during training phase}
	\label{tab:trnphs}
	\begin{tabular}{l|l}
	\hline
	Item & Value \\
	\hline
	Optimizer & Adam \\
	Learning Rate & $10^{-4}$ \\
	Area Batch Size for FPN & 4 \\
	Building Batch Size for PolygonRNN & 12 \\
	\hline
	\end{tabular}
\end{table}

Furthermore, buildings near the edges of area images are typically ignored when training since they are more likely to be incomplete. In this case, the polygon would be clipped by the edges, resulting in new vertices, which can be very hard to compute. Therefore, in this project, two different kinds of ground truth dataset are collected separately, which means that in a specific round of training, the buildings come from the dataset instead of patches from area image.  Training in this way can keep the batch size of buildings unchanged.

\subsection{Prediction Phase}\label{prdphs}

In the prediction phase, the beam search algorithm is introduced. It is a heuristic graph search algorithm, which is usually used when the solution space of a graph is relatively large. In order to reduce the space and time occupied by the search, at each step of depth expansion, some poor quality knots are cut off, and higher quality nodes are kept. This process can significantly reduce space consumption and improve time efficiency, but the disadvantage is that there may be potentially optimal solutions discarded. Therefore, the search result is not necessarily the optimal solution. In beam search, the beam width refers to the number of nodes to be kept and in our project, the beam width is set to be 6. Figure \ref{fig:egbmsrch} shows a single step of the beam search algorithm with beam width 3.

\input{fig/4-05}

\section{Experiment Results}\label{expres}

\subsection{Single Building Segmentation}

\subsection{Buildings Localization}

\subsection{R-PolygonRNN}
